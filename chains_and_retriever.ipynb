{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "770203e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader # loader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # splitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings # vectorizer\n",
    "from langchain_community.vectorstores import FAISS # vector stores (database)\n",
    "from langchain_community.llms import Ollama # local (LLM)\n",
    "from langchain_core.prompts import ChatPromptTemplate # basic prompt designing (one-shot prompting)\n",
    "\n",
    "# CHAINs and RETRIEVERs\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9da71d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pdf\n",
    "pdf_loader = PyPDFLoader('./article.pdf')\n",
    "docs = pdf_loader.load()\n",
    "\n",
    "# splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# vector database\n",
    "gemma_2b_llm = OllamaEmbeddings(model = 'gemma:2b')\n",
    "db = FAISS.from_documents(documents, gemma_2b_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7823d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup local ollama (gemma 2b) model \n",
    "gemma_llm = Ollama(model = 'gemma:2b')\n",
    "\n",
    "# create prompt design\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before providing a detailed answer. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "# document_chaining\n",
    "document_chain = create_stuff_documents_chain(gemma_llm, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026751dd",
   "metadata": {},
   "source": [
    "### How is this working?\n",
    "\n",
    "1. First db.as_retriever() create a fast retrieval mechanism to retrieve from the vector stores 'faiss' for the given input as defined by {input} unstructured query. \n",
    "\n",
    "2. The retrieved documents from the RETRIEVER is then passed as context in the {context} field in the prompt design, along with the {input} query.\n",
    "\n",
    "3. Using LLM, the prompt with context (docs retreived from the vector embedding saved faiss vector store) and input from the user, the LLM generates the response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7363df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrievers\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# retrieval chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9391083",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'what is the performance metric that this works has come up with ?'\n",
    "response = retrieval_chain.invoke({'input': input_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97807b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
